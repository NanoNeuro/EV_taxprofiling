{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../results_diversity', exist_ok=True)\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "os.makedirs(f'../results_diversity/{today}', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINOR PROCESSING FUNCTIONS\n",
    "def process_df(df):\n",
    "    lineage_vals = df.lineage.values\n",
    "    new_lineage_vals = []\n",
    "    for lineage in lineage_vals:\n",
    "        val = ';'.join(lineage.split(';')[::-1]).replace('root;', '').replace('cellular organisms;', '')\n",
    "        new_lineage_vals.append(val)\n",
    "    df.lineage = new_lineage_vals\n",
    "    return df\n",
    "\n",
    "def get_FASTQ_len(POOL_list):\n",
    "    dict_FASTQ_len = {}\n",
    "    \n",
    "    for POOL in POOL_list:\n",
    "        n_counts_fastq = !zcat ../results_rnaseq/star_salmon/unmapped/{POOL}.unmapped_1.fastq.gz | wc -l\n",
    "        dict_FASTQ_len[POOL] = int(n_counts_fastq[0])\n",
    "    \n",
    "    return dict_FASTQ_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAJOR PROCESSING FUNCTIONS\n",
    "\n",
    "def create_POOL_table(POOL, cols_POOL, cutoff_NA, cutoff_CV, cutoff_min_reads):\n",
    "    index_df_POOL = [] # index of taxonomy members to create a merged table with all methods\n",
    "    list_df_pools = []\n",
    "\n",
    "    # Loading the tables. We (1) reverse the phylogenetic lineage to start by root and and by species, (2) sort index by taxonomy and (3) rename count column to the method\n",
    "    for tax_method in cols_POOL:\n",
    "        df_POOL_method = process_df(pd.read_csv(f'../results_profiling/{tax_method}/{POOL}.report.standardised', sep='\\t', index_col='taxonomy_id')).rename(columns={'count': tax_method})\n",
    "        list_df_pools.append(df_POOL_method)\n",
    "        index_df_POOL += df_POOL_method.index.tolist()\n",
    "    \n",
    "    index_df_POOL = list(set(index_df_POOL))\n",
    "    \n",
    "    # Creating the table and merging count columns\n",
    "    df_POOL = pd.DataFrame(index=index_df_POOL, columns= ['name', 'lineage'] + cols_POOL)\n",
    "\n",
    "    for df_POOL_x, name_x in zip(list_df_pools, cols_POOL):\n",
    "        df_POOL.loc[df_POOL_x.index, ['name', 'lineage', name_x]] = df_POOL_x.loc[df_POOL_x.index, ['name', 'lineage', name_x]]\n",
    "\n",
    "    # Apply normalisation based on other pools - this step takes the number of reads of the FASTQs and corrects the counts, so that a a FASTQ with more total reads has fewer normalised counts\n",
    "    cols_POOL_norm = [f'{i}_norm' for i in cols_POOL]\n",
    "    mean_FASTQ_len = np.mean(np.array(list(dict_FASTQ_len.values())))\n",
    "    correction_factor = mean_FASTQ_len / dict_FASTQ_len[POOL]\n",
    "\n",
    "    for col_POOL in cols_POOL:\n",
    "        df_POOL[f'{col_POOL}_norm'] = df_POOL[col_POOL] * correction_factor\n",
    "\n",
    "    # Calculate simple stats\n",
    "    df_POOL.loc[:, 'mean'] = np.mean(df_POOL.loc[:, cols_POOL_norm], axis=1).astype(float)\n",
    "    df_POOL.loc[:, 'std'] = np.std(df_POOL.loc[:, cols_POOL_norm], axis=1)\n",
    "    df_POOL.loc[:, 'CV'] = df_POOL.loc[:, 'std'] / df_POOL.loc[:, 'mean']\n",
    "\n",
    "    # Calculate the number of reads in the fastq to obtain the relative abundance of the counts\n",
    "    df_POOL.loc[:, 'mean (%)'] = 100 * df_POOL.loc[:, 'mean'] / mean_FASTQ_len\n",
    "\n",
    "\n",
    "    df_POOL = df_POOL.sort_values(by='mean', ascending=False)\n",
    "\n",
    "\n",
    "    \n",
    "    # We use some quality metrics to flag and remove \"bad quality\" samples:\n",
    "    #   cutoff_CV to flag species that have very variable counts across profilers\n",
    "    #   cutoff_min_reads and cutoff_min_sum_reads to flag species that have a low count in one and in all profiling counts\n",
    "    #   cutoff_NA to remove species that are only present in 2 of fewer samples\n",
    "    cutoff_sum_reads = int(cutoff_min_reads * len(cols_POOL) * 0.7)\n",
    "    df_POOL[['quality_CV', 'quality_min_reads', 'quality_sum_reads', 'quality_NA']] = 0\n",
    "\n",
    "    df_POOL.loc[df_POOL.loc[:, 'CV'] > cutoff_CV, 'quality_CV'] += 1\n",
    "    df_POOL.loc[df_POOL.loc[:, cols_POOL].min(1) < cutoff_min_reads, 'quality_min_reads'] +=  1\n",
    "    df_POOL.loc[df_POOL.loc[:, cols_POOL].sum(1) < cutoff_sum_reads, 'quality_sum_reads'] += 1\n",
    "    df_POOL.loc[df_POOL.loc[:, cols_POOL].isna().sum(1) > cutoff_NA, 'quality_NA'] += 1\n",
    "    df_POOL['quality'] = df_POOL.loc[:, ['quality_CV', 'quality_min_reads', 'quality_sum_reads', 'quality_NA']].sum(1)\n",
    "    df_POOL = df_POOL[~ np.isnan(df_POOL['CV'])]\n",
    "\n",
    "    # We select species with 0 or 1 flag. We are restrictive in that sense to avoid flagging \"low quality\" species\n",
    "    df_POOL_cutoff = df_POOL[df_POOL['quality'] < 2] \n",
    "\n",
    "\n",
    "    df_POOL.to_csv(f'../results_diversity/{today}/{POOL}.diversity_raw.tsv', sep='\\t')\n",
    "    df_POOL_cutoff .to_csv(f'../results_diversity/{today}/{POOL}.diversity_cutoff.tsv', sep='\\t')\n",
    "\n",
    "    return df_POOL, df_POOL_cutoff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING FUNCTIONS\n",
    "\n",
    "def plot_allPOOL_correlations(POOL_list, cols_POOL, corr_method='spearman'):\n",
    "    ncols = int(len(POOL_list) ** 0.5)\n",
    "    nrows = int(len(POOL_list) // ncols) + int(len(POOL_list) % ncols != 0)\n",
    "\n",
    "    \n",
    "\n",
    "    list_mean_heatmaps = []\n",
    "\n",
    "    for type_plot_idx, type_plot in enumerate(['raw', 'cutoff']):\n",
    "        _, axs= plt.subplots(nrows, ncols, figsize=(4 * ncols, 4 * nrows))\n",
    "        corr_mat_list = [] # this is to later sum all correlations and do a big plot\n",
    "        for ax_int, POOL in enumerate(POOL_list):\n",
    "            df_POOL = pd.read_csv(f'../results_diversity/{today}/{POOL}.diversity_{type_plot}.tsv', sep='\\t')\n",
    "            df_corr = np.log10(df_POOL.loc[:, cols_POOL].astype(float) + 1)\n",
    "            corr_mat = df_corr.corr(method=corr_method)\n",
    "            corr_mat_list.append(corr_mat)\n",
    "\n",
    "            sns.heatmap(corr_mat, cmap='Blues', annot=True, ax=axs.ravel()[ax_int])\n",
    "            axs.ravel()[ax_int].set_title(POOL)\n",
    "        \n",
    "        plt.suptitle(f'Correlation ({corr_method}, {type_plot})')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(f'../results_diversity/{today}/correlation_{corr_method}_{type_plot}.png', dpi=300)\n",
    "\n",
    "        # Create the mean heatmap\n",
    "        mean_ht = corr_mat_list[0]\n",
    "        for corr_mat in corr_mat_list[1:]:\n",
    "            mean_ht += corr_mat\n",
    "        mean_ht /= len(corr_mat_list)\n",
    "\n",
    "        list_mean_heatmaps.append(mean_ht)\n",
    "\n",
    "\n",
    "    _, axs_all = plt.subplots(1, 2, figsize=(4 * 2, 4 * 1))\n",
    "\n",
    "    for type_plot_idx, type_plot in enumerate(['raw', 'cutoff']):\n",
    "        sns.heatmap(list_mean_heatmaps[type_plot_idx], cmap='Blues', annot=True, ax=axs_all.ravel()[type_plot_idx])\n",
    "        axs_all.ravel()[type_plot_idx].set_title(type_plot)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../results_diversity/{today}/correlation_{corr_method}_mean.png', dpi=300)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL VARIABLES\n",
    "POOL_list = [f'POOL{i}' for i in range(1, 13)]\n",
    "dict_FASTQ_len = get_FASTQ_len(POOL_list)\n",
    "dict_FASTQ_len\n",
    "\n",
    "cols_POOL = ['kaiju', 'kraken_2', 'krakenuniq', 'centrifuge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pool in POOL_list:\n",
    "    _, _ = create_POOL_table(pool, cols_POOL, cutoff_NA=1, cutoff_CV=1.0, cutoff_min_reads=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_allPOOL_correlations(POOL_list, cols_POOL, corr_method='spearman')\n",
    "\n",
    "# FALLABA AQUI Y NO SE POR QUE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EVs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
