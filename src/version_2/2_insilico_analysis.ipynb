{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "plt.rcParams['figure.dpi']=170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from list_vars import LIST_PROFILERS, DIR_FIGURES, RESULTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In silico sample analysis\n",
    "\n",
    "In this notebook we are going to do an analysis on the *in silico* samples, where we are going to study several variables.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many reads are incorrectly mapped if we do not perfom a host mapping step?\n",
    "\n",
    "It has been reported that not mapping to human databases before profiling increases the number of reads assigned to other organisms. \n",
    "\n",
    "In this case, we are going to do 3 checks with the *in silico* dataset using pass2 (profiling after 2-time host mapping) and pass0 (direct profiling withou host mapping), and we are going to check the influence in parameter sensitivity:\n",
    "-  We are going to see what is the total number of reads mapped to the human dataset, and what is the offset left unmapped which should have been mapped to human.\n",
    "    - We are also going to do the same with the microbial reads, and see if more microbial reads have been assigned to the pass0 dataset.\n",
    "\n",
    "Later in the analysis we are going to do two additional analyses:\n",
    "-  We are going to see the number of species present in total between pass0 and pass2, and their jaccard index.\n",
    "- We are going to calculate the ratio between the number of reads in pass0 and pass2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_host_map_info = pd.read_csv(f'{RESULTS_DIR}/counts/mapping_counts.txt', sep='\\t').set_index('SAMPLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_taxid_counts = pd.read_csv('table_artificial_taxid.csv', sep=';', names=['species', 'taxid', 'reads'])\n",
    "artificial_taxid_counts['reads_true'] = (artificial_taxid_counts['reads'] / 2).astype(int)\n",
    "\n",
    "n_true_human_reads = int(artificial_taxid_counts['reads_true'].iloc[0])\n",
    "n_true_human_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mapped_reads_1and2_maps = df_host_map_info.loc['ARTIFICIAL', '1st_mapped'] + df_host_map_info.loc['ARTIFICIAL', '2nd_mapped']\n",
    "\n",
    "print(f'There is a total of {n_mapped_reads_1and2_maps} reads mapped to human during the 1st and 2nd map, which represents around {100 * n_mapped_reads_1and2_maps/n_true_human_reads} % of the total number of reads ({n_true_human_reads}).')\n",
    "print(f'There is a total of {n_true_human_reads - n_mapped_reads_1and2_maps} reads remaining to be mapped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_host_profile_info = pd.read_csv(f'{RESULTS_DIR}/counts/profiling_counts_ARTIFICIAL.txt', sep='\\t')\n",
    "df_host_profile_info_artificial = df_host_profile_info[df_host_profile_info['SAMPLE'] == 'ARTIFICIAL']\n",
    "\n",
    "df_host_profile_info_artificial['mapped_human_1_2_maps'] = 0\n",
    "df_host_profile_info_artificial.loc[df_host_profile_info_artificial['pass'] == 2, 'mapped_human_1_2_maps'] = n_mapped_reads_1and2_maps\n",
    "\n",
    "df_host_profile_info_artificial['mapped_human_total'] = df_host_profile_info_artificial['mapped_human_1_2_maps'] + df_host_profile_info_artificial['mapped_human']\n",
    "df_host_profile_info_artificial['total_reads'] = df_host_profile_info_artificial['mapped_human_total'] + df_host_profile_info_artificial['mapped_others'] + df_host_profile_info_artificial['unmapped']\n",
    "\n",
    "df_host_profile_info_artificial['observed_human_prop'] = df_host_profile_info_artificial['mapped_human_total'] / df_host_profile_info_artificial['total_reads']\n",
    "df_host_profile_info_artificial['observed_others_prop'] = df_host_profile_info_artificial['mapped_others'] / df_host_profile_info_artificial['total_reads']\n",
    "df_host_profile_info_artificial['observed_unmapped_prop'] = df_host_profile_info_artificial['unmapped'] / df_host_profile_info_artificial['total_reads']\n",
    "\n",
    "df_host_profile_info_artificial['expected_human_prop'] = n_true_human_reads / artificial_taxid_counts['reads_true'].sum() # 0.8\n",
    "df_host_profile_info_artificial['expected_others_prop'] = 1 - n_true_human_reads / artificial_taxid_counts['reads_true'].sum() # 0.8\n",
    "\n",
    "df_host_profile_info_artificial['calculated_unmapped_human_prop'] = df_host_profile_info_artificial['expected_human_prop'] - df_host_profile_info_artificial['observed_human_prop']\n",
    "df_host_profile_info_artificial['calculated_unmapped_others_prop'] = df_host_profile_info_artificial['expected_others_prop'] - df_host_profile_info_artificial['observed_others_prop']\n",
    "\n",
    "df_host_profile_info_artificial['proportion_mapped_other_reads'] = df_host_profile_info_artificial['observed_others_prop'] /  df_host_profile_info_artificial['expected_others_prop']\n",
    "\n",
    "\n",
    "for profiler in LIST_PROFILERS:\n",
    "    display(profiler)\n",
    "    display(df_host_profile_info_artificial[df_host_profile_info_artificial['profiler'] == profiler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1A) Check if there are differences in human read assignment.\n",
    "\n",
    "# Step 1: Calculate the differences between pass 2 and pass 0 for each profiler and mode\n",
    "\n",
    "pass_diff = (\n",
    "    df_host_profile_info_artificial.pivot_table(\n",
    "        index=[\"profiler\", \"mode\"], columns=\"pass\", values=\"observed_human_prop\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Ensure column names are integers\n",
    "pass_diff.columns.name = None  # Remove the columns' name from pivot_table\n",
    "pass_diff.columns = ['profiler', 'mode', 0, 2]  # Explicitly rename columns\n",
    "\n",
    "# Calculate the difference\n",
    "pass_diff[\"difference\"] = 100 * (pass_diff[2] - pass_diff[0])\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Plot the differences using a lineplot\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.lineplot(\n",
    "    data=pass_diff,\n",
    "    x=\"mode\",\n",
    "    y=\"difference\",\n",
    "    hue=\"profiler\",\n",
    "    marker=\"o\",\n",
    "    palette=\"tab10\",\n",
    ")\n",
    "\n",
    "plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "plt.title(\"Difference in Observed Human Proportion (Pass 2 - Pass 0)\", fontsize=14)\n",
    "plt.xlabel(\"Mode\", fontsize=12)\n",
    "plt.ylabel(\"Diff (%)\", fontsize=12)\n",
    "\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.legend(title=\"Profiler\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1B) Check if there are differences in non-human read assignment.\n",
    "\n",
    "pass_diff = (\n",
    "    df_host_profile_info_artificial.pivot_table(\n",
    "        index=[\"profiler\", \"mode\"], columns=\"pass\", values=\"observed_others_prop\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "pass_diff[\"difference\"] = 100 * (pass_diff[2] - pass_diff[0])\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Plot the differences using a lineplot\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.lineplot(\n",
    "    data=pass_diff,\n",
    "    x=\"mode\",\n",
    "    y=\"difference\",\n",
    "    hue=\"profiler\",\n",
    "    marker=\"o\",\n",
    "    palette=\"tab10\",\n",
    ")\n",
    "\n",
    "plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "plt.title(\"Difference in Observed Non-Human Proportion (Pass 2 - Pass 0)\", fontsize=14)\n",
    "plt.xlabel(\"Mode\", fontsize=12)\n",
    "plt.ylabel(\"Diff (%)\", fontsize=12)\n",
    "\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.legend(title=\"Profiler\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see here?**\n",
    "- The number of reads assigned to humans without host mapping is very variable depending on the profiler. Centrifuge, krakenuniq and ganon and map human reads correctly, whereas kaiju, kraken2 fail to map the reads to human. The differences tend to decrease with the sensitivity mode, that is, paradoxically, a more strict read assignment leads to an improved number of human-mapped reads. However, this makes sense because more reads are assigned in general, and thus both human and non-human reads are mapped.\n",
    "- However, this difference does not occur in non-human species. In general, non-human species are assigned equally with or without host mapping. This is interesting because we would expect a higher amount of reads assigned to non-human species originating from a false positive assignment of human reads, but seems not to be the case, even in profilers that have a high ammount of unmapped human reads.\n",
    "    - Still, we have to take into acount that the profiler databases include a host mapping step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_artificial_taxcounts = pd.read_csv('../../src/version_2/table_artificial_taxid.csv', sep=';', names=['species', 'taxid', 'count'])\n",
    "table_artificial_taxcounts = table_artificial_taxcounts[table_artificial_taxcounts['taxid'] != 9606]\n",
    "table_artificial_taxcounts['abundance'] = 100 * table_artificial_taxcounts['count'] / table_artificial_taxcounts['count'].sum()\n",
    "table_artificial_taxcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing detection stats to aswer the questions\n",
    "\n",
    "One of the parameters used during profiling is the mode of the profilers. Each profiler has a different set of parametters to include reads as valid or not. This may results in the detection of false positives and negatives. \n",
    "\n",
    "Here, we are going to study this effect in *in silico* samples to see if there are major changes. We are can measure the effectivity of several variables: \n",
    "- Categorical values: we can use each of the columns in the flag system to check how well were species assigned. We can use the precision (TP/TP + FP), recall (TP/TP+FN) and F1-score (2 x precision x recall / precision + recall) and Cohen's kappa.\n",
    "$$\\kappa = \\frac{p_0-p_e}{1-p_e} \\quad p_0 = \\frac{TP + TN}{TP + FP + FN + TN} \\quad p_e=\\frac{TP + FP}{TP + FP + FN + TN}\\cdot\\frac{TP + FN }{TP + FP + FN + TN} + \\frac{TN + FP}{TP + FP + FN + TN}\\cdot\\frac{TN + FN}{TP + FP + FN + TN}$$\n",
    "\n",
    "- Numerical values: we can use the normalized value and the abundance to see how well are reads classified. For that we can use the expected number of reads and abundance. With that we will calculate the (1) difference between observed and expected categories and (2) the mean absolute error:\n",
    "$$(1) \\qquad DIFF_i = 100\\cdot\\frac{x_{obs,i} - x_{exp,i}}{x_{exp,i}}$$ \n",
    "$$(2) \\qquad MAE = \\frac{100}{N}\\sum\\frac{x_{obs,i} - x_{exp,i}}{x_{exp,i}}$$ \n",
    "\n",
    "- For numerical values we are also going to calculate the pearson correlation between the observed and expected values, using a log10(1+x) transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nominal_metrics(df_tax_ground_truth, df_flags_observed, column):\n",
    "    list_expected_taxids = list(df_tax_ground_truth['taxid'].astype(int).values)\n",
    "    list_observed_true_taxids = list(df_flags_observed.loc[df_flags_observed[column] == False, 'taxonomy_id'].astype(int).values)\n",
    "    list_observed_false_taxids = list(df_flags_observed.loc[df_flags_observed[column] == True, 'taxonomy_id'].astype(int).values)\n",
    "\n",
    "    TP = len([i for i in list_expected_taxids if i in list_observed_true_taxids])\n",
    "    FN = len([i for i in list_expected_taxids if i not in list_observed_true_taxids])\n",
    "    FP = len([i for i in list_observed_true_taxids if i not in list_expected_taxids])\n",
    "    TN = len([i for i in list_observed_false_taxids if i not in list_expected_taxids])\n",
    "\n",
    "    assert len(set(list_expected_taxids + list_observed_true_taxids + list_observed_false_taxids)) == TP + FN + FP + TN\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "\n",
    "    try:\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "    except:\n",
    "        f1 = 0\n",
    "    \n",
    "    # Create kappa measures\n",
    "    ALL = TP + FN + FP + TN\n",
    "    p0 = (TP + TN) / (ALL)\n",
    "    pe = (TP + FP)/ALL * (TP + FN)/ALL + (TN + FP)/ALL * (TN + FN)/ALL\n",
    "    kappa = (p0 - pe) / (1 - pe)\n",
    "\n",
    "    return precision, recall, f1, kappa, TP, FN, FP, TN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_selected = ['centrifuge_norm', 'ganon_norm', 'kaiju_norm', 'kmcp_norm', 'kraken2_norm', 'krakenuniq_norm',\n",
    "                    'centrifuge_relab', 'ganon_relab', 'kaiju_relab', 'kmcp_relab', 'kraken2_relab', 'krakenuniq_relab',\n",
    "                    'mean_norm', 'CV_norm', 'mean_relab', 'CV_relab']\n",
    "df_nominal_stats = {'pass': [], 'mode': [], 'S': [], 'column': [], 'precision': [], 'recall': [], 'f1': [], \n",
    "                    'kappa': [], 'TP|FN|FP|TN': []}\n",
    "\n",
    "for passn in [0, 2]:\n",
    "    for mode in range(1, 10):\n",
    "        for S in [0, 1, 2, 3, 4, 5, 6, 7, 10, 15]:\n",
    "            for column in columns_selected: \n",
    "                summary_table_flags = pd.read_csv(f'{RESULTS_DIR}/summary/ARTIFICIAL_pass{passn}_mode{mode}_taxspecies_S{S}.flags.tsv', sep='\\t')\n",
    "                try:\n",
    "                    precision, recall, f1, kappa, TP, FN, FP, TN = calculate_nominal_metrics(table_artificial_taxcounts, summary_table_flags, column)\n",
    "                except KeyError:\n",
    "                    continue \n",
    "\n",
    "                df_nominal_stats['pass'].append(passn)\n",
    "                df_nominal_stats['mode'].append(mode)\n",
    "                df_nominal_stats['S'].append(S)\n",
    "                df_nominal_stats['column'].append(column)\n",
    "\n",
    "                df_nominal_stats['precision'].append(precision)\n",
    "                df_nominal_stats['recall'].append(recall)\n",
    "                df_nominal_stats['f1'].append(f1)\n",
    "                df_nominal_stats['kappa'].append(kappa)\n",
    "                df_nominal_stats['TP|FN|FP|TN'].append((TP, FN, FP, TN))\n",
    "\n",
    "df_nominal_stats = pd.DataFrame(df_nominal_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mad(values):\n",
    "    median = np.median(values)\n",
    "    mad = np.median(np.abs(values - median))\n",
    "    return mad\n",
    "\n",
    "def calculate_numerical_metrics(df_tax_ground_truth, df_counts_observed, df_flags_observed, profiler, suffix):\n",
    "    df_tax_ground_truth = df_tax_ground_truth.copy().set_index('taxid')\n",
    "    df_counts_observed = df_counts_observed.copy().set_index('taxonomy_id')\n",
    "    df_flags_observed = summary_table_flags.set_index('taxonomy_id').copy()\n",
    "\n",
    "    list_expected_taxids = df_tax_ground_truth.index.astype(int).values\n",
    "    list_observed_true_taxids = df_flags_observed.loc[df_flags_observed[f'{profiler}_{suffix}'].astype(bool) == False].index.astype(int).values\n",
    "\n",
    "    combined_taxid = np.intersect1d(list_expected_taxids, list_observed_true_taxids)\n",
    "    species = df_tax_ground_truth.loc[combined_taxid, 'species'].values\n",
    "    observed_counts = df_counts_observed.loc[combined_taxid, f'{profiler}_{suffix}'].values\n",
    "\n",
    "    expected_col = 'count' if suffix == 'norm' else 'abundance'\n",
    "    expected_counts = df_tax_ground_truth.loc[combined_taxid, expected_col].values\n",
    "\n",
    "    diff_counts = 100 * (observed_counts - expected_counts) / expected_counts\n",
    "\n",
    "    MAE_counts = np.mean(diff_counts)\n",
    "    MAED_counts = np.std(diff_counts)\n",
    "    MACV_counts = MAED_counts / MAE_counts\n",
    "\n",
    "    if len(combined_taxid) > 5:\n",
    "        if suffix == 'norm':\n",
    "            corr, _ = pearsonr(np.log10(1 + observed_counts), np.log10(1 + expected_counts))\n",
    "            rmse = np.sqrt(np.mean((np.log10(1 + observed_counts) - np.log10(1 + expected_counts)) ** 2))\n",
    "        else:\n",
    "            corr, _ = pearsonr(observed_counts, expected_counts)\n",
    "            rmse = np.sqrt(np.mean((observed_counts - expected_counts) ** 2))\n",
    "    else:\n",
    "        corr, rmse = np.nan, np.nan\n",
    "\n",
    "    return diff_counts, MAE_counts, MAED_counts, MACV_counts, corr, rmse, combined_taxid, species, expected_counts, observed_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical_stats = {'pass': [], 'mode': [], 'S': [], 'profiler': [], \n",
    "                    'diff_counts': [], 'MAE_counts': [], 'MAED_counts': [], 'MACV_counts': [], \n",
    "                    'corr_counts': [], 'RMSE_counts': [], 'taxid_counts': [], \n",
    "                    'species_counts': [], 'expected_counts': [], 'observed_counts': [], \n",
    "                    'diff_abundance': [], 'MAE_abundance': [], 'MAED_abundance': [], 'MACV_abundance': [], \n",
    "                    'corr_abundance': [], 'RMSE_abundance': [], 'taxid_abundance': [], \n",
    "                    'species_abundance': [], 'expected_abundance': [], 'observed_abundance': [],  }\n",
    "\n",
    "for passn in [0, 2]:\n",
    "    for mode in range(1, 10):\n",
    "        for S in [0, 1, 2, 3, 4, 5, 6, 7, 10, 15]:\n",
    "            for profiler in LIST_PROFILERS + ['mean']: \n",
    "                summary_table_flags = pd.read_csv(f'{RESULTS_DIR}/summary/ARTIFICIAL_pass{passn}_mode{mode}_taxspecies_S{S}.flags.tsv', sep='\\t')\n",
    "                summary_table_counts = pd.read_csv(f'{RESULTS_DIR}/summary/ARTIFICIAL_pass{passn}_mode{mode}_taxspecies_S{S}.diversity.tsv', sep='\\t')\n",
    "                try:\n",
    "                    diff_counts, MAE_counts, MAED_counts, MACV_counts, corr_counts, rmse_counts, taxids_counts, species_counts, expected_counts, observed_counts = \\\n",
    "                        calculate_numerical_metrics(table_artificial_taxcounts, summary_table_counts, \\\n",
    "                                                                        summary_table_flags, profiler, suffix='norm')\n",
    "                    diff_abundance, MAE_abundance, MAED_abundance, MACV_abundance, corr_abundance, rmse_abundance, taxids_abundance, species_abundance, expected_abundance, observed_abundance = \\\n",
    "                        calculate_numerical_metrics(table_artificial_taxcounts, summary_table_counts, \\\n",
    "                                                                        summary_table_flags, profiler, suffix='relab')\n",
    "                except KeyError:\n",
    "                    continue \n",
    "\n",
    "                df_numerical_stats['pass'].append(passn)\n",
    "                df_numerical_stats['mode'].append(mode)\n",
    "                df_numerical_stats['S'].append(S)\n",
    "                df_numerical_stats['profiler'].append(profiler)\n",
    "\n",
    "                df_numerical_stats['diff_counts'].append(diff_counts)\n",
    "                df_numerical_stats['MAE_counts'].append(MAE_counts)                \n",
    "                df_numerical_stats['MAED_counts'].append(MAED_counts)                \n",
    "                df_numerical_stats['MACV_counts'].append(MACV_counts)                \n",
    "                df_numerical_stats['corr_counts'].append(corr_counts)                \n",
    "                df_numerical_stats['RMSE_counts'].append(rmse_counts) \n",
    "                df_numerical_stats['taxid_counts'].append(taxids_counts)\n",
    "                df_numerical_stats['species_counts'].append(species_counts)\n",
    "                df_numerical_stats['expected_counts'].append(expected_counts)\n",
    "                df_numerical_stats['observed_counts'].append(observed_counts)\n",
    "\n",
    "                df_numerical_stats['diff_abundance'].append(diff_abundance)\n",
    "                df_numerical_stats['MAE_abundance'].append(MAE_abundance)\n",
    "                df_numerical_stats['MAED_abundance'].append(MAED_abundance)\n",
    "                df_numerical_stats['MACV_abundance'].append(MACV_abundance)\n",
    "                df_numerical_stats['corr_abundance'].append(corr_abundance)                \n",
    "                df_numerical_stats['RMSE_abundance'].append(rmse_abundance) \n",
    "                df_numerical_stats['taxid_abundance'].append(taxids_abundance)\n",
    "                df_numerical_stats['species_abundance'].append(species_abundance)\n",
    "                df_numerical_stats['expected_abundance'].append(expected_abundance)\n",
    "                df_numerical_stats['observed_abundance'].append(observed_abundance)\n",
    "\n",
    "df_numerical_stats = pd.DataFrame(df_numerical_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of kappa/F1\n",
    "\n",
    "F1-score and $\\kappa$ are quite different measures but we observe that they are correlated in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot with the identity line\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='f1', y='kappa', data=df_nominal_stats, label='Data Points')\n",
    "\n",
    "# Add the identity line (y = x)\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Identity Line')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('F1-score vs Kappa')\n",
    "plt.xlabel('F1-score')\n",
    "plt.ylabel('Kappa')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute and print correlation (Pearson by default)\n",
    "corr = df_nominal_stats['f1'].corr(df_nominal_stats['kappa'])\n",
    "print(\"Pearson correlation between F1 and Kappa:\", corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that sense, we can then use one of the measures to explain the results and don't need the second one. We are going to select the F1 score because it has a more clear interpretability and it is related to precision and recall, which are alrady being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the S parametter used during curve fitting affect?\n",
    "\n",
    "The S parametter is useful to tweak the detection results, so that we can include more or less species during the flagging step. Since it is a structural parametter, we want to fit it first so that we can answer several other comparisons.\n",
    "\n",
    "To do this we are going to use the nominal variables and their derived statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking recall/precision/F1-score for inclusion/exclusion of species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [f'{i}_norm' for i in LIST_PROFILERS] + ['mean_norm']\n",
    "modes = range(2, 9)\n",
    "passn = [2]\n",
    "S_values = df_nominal_stats['S'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = df_nominal_stats[(df_nominal_stats['pass'].isin(passn)) & \\\n",
    "                             (df_nominal_stats['column'].isin(cols)) & \\\n",
    "                              (df_nominal_stats['mode'].isin(modes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = pd.melt(\n",
    "    subset_df,\n",
    "    id_vars=['mode', 'S', 'column'],\n",
    "    value_vars=['recall', 'precision', 'f1'],\n",
    "    var_name='metric',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Create a colormap for 'mode'\n",
    "norm = Normalize(vmin=melted_df['mode'].min(), vmax=melted_df['mode'].max())\n",
    "cmap = plt.cm.viridis  # Choose a colormap (e.g., 'viridis', 'plasma', 'cividis')\n",
    "\n",
    "# Create a FacetGrid: 6x3 grid (row for each profiler, column for each metric)\n",
    "g = sns.FacetGrid(\n",
    "    melted_df, \n",
    "    col='column', \n",
    "    row='metric', \n",
    "    height=3, \n",
    "    sharey=True, \n",
    "    sharex=True\n",
    ")\n",
    "\n",
    "# Map the lineplot to the grid\n",
    "def lineplot_with_cmap(data, **kwargs):\n",
    "    for mode in sorted(data['mode'].unique()):\n",
    "        subset = data[data['mode'] == mode]\n",
    "        plt.plot(subset['S'], subset['score'], label=f\"Mode {mode}\",\n",
    "                 color=cmap(norm(mode)), marker='o')\n",
    "\n",
    "g.map_dataframe(lineplot_with_cmap)\n",
    "\n",
    "# Create a legend for the discrete modes\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], color=cmap(norm(mode)), marker='o', linestyle='', label=f\"Mode {mode}\")\n",
    "    for mode in sorted(melted_df['mode'].unique())\n",
    "]\n",
    "plt.legend(\n",
    "    handles=handles, \n",
    "    title=\"\", \n",
    "    bbox_to_anchor=(1.05, 3), \n",
    "    loc='center left', \n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "# Set x-axis ticks (if you have specific S values)\n",
    "g.set(xticks=subset_df['S'].unique())\n",
    "\n",
    "for ax in g.axes.ravel():\n",
    "    ax.set_title('')\n",
    "\n",
    "# Add axis labels and titles\n",
    "for ax, profiler in zip(g.axes[0, :], melted_df['column'].unique()):\n",
    "    ax.set_title(profiler.replace('_norm', ''))\n",
    "\n",
    "for ax, score in zip(g.axes[:, 0], ['recall', 'precision', 'F1-score']):\n",
    "    ax.set_ylabel(score)\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"Metrics by Profiler and Mode\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(\n",
    "    melted_df, \n",
    "    col='column', \n",
    "    row='metric', \n",
    "    height=3, \n",
    "    sharey=True, \n",
    "    sharex=True\n",
    ")\n",
    "\n",
    "g.map(sns.boxplot, 'S', 'score')\n",
    "\n",
    "# Set x-axis ticks (if you have specific S values)\n",
    "for ax in g.axes.ravel():\n",
    "    ax.set_title('')\n",
    "\n",
    "# Add axis labels and titles\n",
    "for ax, profiler in zip(g.axes[0, :], melted_df['column'].unique()):\n",
    "    ax.set_title(profiler.replace('_norm', ''))\n",
    "\n",
    "for ax, score in zip(g.axes[:, 0], ['recall', 'precision', 'F1-score']):\n",
    "    ax.set_ylabel(score)\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"Metrics by Profiler and Mode\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this part of the analysis was to select the \"optimal\" `S` to then make other comparisons and extract proper conclusions. \n",
    "If we look at individual profilers, the aim is not the select the `S` with best F1 score, but to select the smallest `S` that provides a sufficiently high recall, ensuring that we don't lose TP species. This threshold depends on the profiler. For CEN it is 6-7, GAN is 7-10, KAI is 1-2, KR2 is 4-5, KRU is 5-6. We see that at these values the precision drops (expectedly), but it remains stable afterwards for most profilers. Therefore, a value of `S=7` should be sufficient to ensure that the results are correct.\n",
    "\n",
    "The advantage of using the mean value instead of the individual profilers is that it tends to retrieve a better stability on the precision throughout the S values and modes. \n",
    "\n",
    "Therefore, we are going to choose S=2 and S=7 for comparisons of robustness with biological samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Does pass0/pass2 (no host pre-mapping vs host pre-mapping) affect the detection of the species?\n",
    "\n",
    "For this part we are going to run run analyses:\n",
    "- Retrieve the raw detection of species with the passes, and calculate their jaccard index.\n",
    "- Calculate the Pearson correlation + RMSE for several mode values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize, ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_nominal_stats[df_nominal_stats['column'].isin([f'{i}_norm' for i in LIST_PROFILERS] + ['mean_norm'])].copy()\n",
    "\n",
    "# Filter rows for pass=0 and pass=2\n",
    "df_pass_0 = df[df['pass'] == 0].set_index(['mode', 'S', 'column'])\n",
    "df_pass_2 = df[df['pass'] == 2].set_index(['mode', 'S', 'column'])\n",
    "\n",
    "# Compute the difference (pass=2 - pass=0)\n",
    "diff_nominal_df = df_pass_2[['precision', 'recall', 'f1']] - df_pass_0[['precision', 'recall', 'f1']]\n",
    "diff_nominal_df = diff_nominal_df.reset_index()\n",
    "\n",
    "# Rename columns to indicate the differences\n",
    "diff_nominal_df.rename(columns={\n",
    "    'precision': 'precision_diff',\n",
    "    'recall': 'recall_diff',\n",
    "    'f1': 'f1_diff'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(diff_nominal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = pd.melt(\n",
    "    diff_nominal_df,\n",
    "    id_vars=['mode', 'S', 'column'],\n",
    "    value_vars=['recall_diff', 'precision_diff', 'f1_diff'],\n",
    "    var_name='metric',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Create a colormap for 'mode'\n",
    "norm = Normalize(vmin=melted_df['mode'].min(), vmax=melted_df['mode'].max())\n",
    "cmap = plt.cm.viridis  # Choose a colormap (e.g., 'viridis', 'plasma', 'cividis')\n",
    "\n",
    "# Create a FacetGrid: 6x3 grid (row for each profiler, column for each metric)\n",
    "g = sns.FacetGrid(\n",
    "    melted_df, \n",
    "    col='column', \n",
    "    row='metric', \n",
    "    height=3, \n",
    "    sharey=True, \n",
    "    sharex=True\n",
    ")\n",
    "\n",
    "# Map the lineplot to the grid\n",
    "def lineplot_with_cmap(data, **kwargs):\n",
    "    for mode in sorted(data['mode'].unique()):\n",
    "        subset = data[data['mode'] == mode]\n",
    "        plt.plot(subset['S'], subset['score'], label=f\"Mode {mode}\",\n",
    "                 color=cmap(norm(mode)), marker='o')\n",
    "\n",
    "g.map_dataframe(lineplot_with_cmap)\n",
    "\n",
    "# Create a legend for the discrete modes\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], color=cmap(norm(mode)), marker='o', linestyle='', label=f\"Mode {mode}\")\n",
    "    for mode in sorted(melted_df['mode'].unique())\n",
    "]\n",
    "plt.legend(\n",
    "    handles=handles, \n",
    "    title=\"\", \n",
    "    bbox_to_anchor=(1.05, 3), \n",
    "    loc='center left', \n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "# Set x-axis ticks (if you have specific S values)\n",
    "g.set(xticks=diff_nominal_df['S'].unique())\n",
    "\n",
    "for ax in g.axes.ravel():\n",
    "    ax.set_title('')\n",
    "\n",
    "# Add axis labels and titles\n",
    "for ax, profiler in zip(g.axes[0, :], melted_df['column'].unique()):\n",
    "    ax.set_title(profiler.replace('_norm', ''))\n",
    "\n",
    "for ax, score in zip(g.axes[:, 0], ['recall', 'precision', 'F1-score']):\n",
    "    ax.set_ylabel(score)\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"Metrics by Profiler and Mode\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical_stats[(df_numerical_stats['S'].isin([10, 15])) & (df_numerical_stats['profiler'] == 'centrifuge')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_numerical_stats.copy()\n",
    "\n",
    "# Filter rows for pass=0 and pass=2\n",
    "df_pass_0 = df[df['pass'] == 0].set_index(['mode', 'S', 'profiler'])\n",
    "df_pass_2 = df[df['pass'] == 2].set_index(['mode', 'S', 'profiler'])\n",
    "\n",
    "# Compute the difference (pass=2 - pass=0)\n",
    "diff_numerical_df = df_pass_2[['MAE_counts',\t'MAED_counts']] - df_pass_0[['MAE_counts',\t'MAED_counts']]\n",
    "diff_numerical_df = diff_numerical_df.reset_index()\n",
    "\n",
    "# Rename columns to indicate the differences\n",
    "diff_numerical_df.rename(columns={\n",
    "    'MAE_counts': 'MAE_diff',\n",
    "    'MAED_counts': 'MAED_diff',\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(diff_numerical_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = pd.melt(\n",
    "    diff_numerical_df,\n",
    "    id_vars=['mode', 'S', 'profiler'],\n",
    "    value_vars=['MAE_diff', 'MAED_diff'],\n",
    "    var_name='metric',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Create a colormap for 'mode'\n",
    "norm = Normalize(vmin=melted_df['mode'].min(), vmax=melted_df['mode'].max())\n",
    "cmap = plt.cm.viridis  # Choose a colormap (e.g., 'viridis', 'plasma', 'cividis')\n",
    "\n",
    "# Create a FacetGrid: 6x3 grid (row for each profiler, column for each metric)\n",
    "g = sns.FacetGrid(\n",
    "    melted_df, \n",
    "    col='profiler', \n",
    "    row='metric', \n",
    "    height=3, \n",
    "    sharey=False, \n",
    "    sharex=True\n",
    ")\n",
    "\n",
    "# Map the lineplot to the grid\n",
    "def lineplot_with_cmap(data, **kwargs):\n",
    "    for mode in sorted(data['mode'].unique()):\n",
    "        subset = data[data['mode'] == mode]\n",
    "        plt.plot(subset['S'], subset['score'], label=f\"Mode {mode}\",\n",
    "                color=cmap(norm(mode)), marker='o')\n",
    "\n",
    "g.map_dataframe(lineplot_with_cmap)\n",
    "\n",
    "# Create a legend for the discrete modes\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], color=cmap(norm(mode)), marker='o', linestyle='', label=f\"Mode {mode}\")\n",
    "    for mode in sorted(melted_df['mode'].unique())\n",
    "]\n",
    "plt.legend(\n",
    "    handles=handles, \n",
    "    title=\"\", \n",
    "    bbox_to_anchor=(1.05, 1.75), \n",
    "    loc='center left', \n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "# Set x-axis ticks (if you have specific S values)\n",
    "g.set(xticks=subset_df['S'].unique())\n",
    "\n",
    "for ax in g.axes.ravel():\n",
    "    ax.set_title('')\n",
    "\n",
    "# Add axis labels and titles\n",
    "for ax, profiler in zip(g.axes[0, :], melted_df['profiler'].unique()):\n",
    "    ax.set_title(profiler.replace('_norm', ''))\n",
    "\n",
    "for ax, score in zip(g.axes[:, 0], ['MAE_diff', 'MAED_diff']):\n",
    "    ax.set_ylabel(score)\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"Metrics by Profiler and Mode\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking at the correlation between the read counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at a concrete example\n",
    "mode = 5\n",
    "S = 10\n",
    "sample = 'ARTIFICIAL'\n",
    "\n",
    "\n",
    "pass0_df = summary_table_counts = pd.read_csv(f'{RESULTS_DIR}/summary/ARTIFICIAL_pass0_mode{mode}_taxspecies_S{S}.diversity.tsv', sep='\\t')\n",
    "pass2_df = summary_table_counts = pd.read_csv(f'{RESULTS_DIR}/summary/ARTIFICIAL_pass2_mode{mode}_taxspecies_S{S}.diversity.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (pass0_df['mean_norm'] > 300) & (pass2_df['mean_norm'] > 300)\n",
    "fig, axs = plt.subplots(1,6, figsize=(18, 3))\n",
    "\n",
    "\n",
    "# Iterate through profilers and calculate Pearson correlation and RMSE\n",
    "for i, profiler in enumerate(LIST_PROFILERS + ['mean']):\n",
    "    # Extract values for pass 0 and pass 2\n",
    "    p0counts = np.log10(1 + np.clip(pass0_df.loc[idx, f'{profiler}_norm'].values, 0, None))\n",
    "    p2counts = np.log10(1 + np.clip(pass2_df.loc[idx, f'{profiler}_norm'].values, 0, None))\n",
    "\n",
    "    # Filter out NaN values\n",
    "    valid_mask = ~np.isnan(p0counts) & ~np.isnan(p2counts)\n",
    "    p0counts = p0counts[valid_mask]\n",
    "    p2counts = p2counts[valid_mask]\n",
    "\n",
    "    # Plot vertical line\n",
    "    axs[i].plot([1, 6], [1, 6], c='#bc0000', linestyle='--', linewidth=1.5)\n",
    "\n",
    "    # Scatterplot\n",
    "    sns.scatterplot(x=p0counts, y=p2counts, ax=axs[i])\n",
    "\n",
    "    # Calculate Pearson correlation\n",
    "    corr, _ = pearsonr(p0counts, p2counts)\n",
    "\n",
    "    # Calculate RMSE (Root Mean Squared Error)\n",
    "    rmse = np.sqrt(np.mean((p0counts - p2counts) ** 2))\n",
    "\n",
    "    print(profiler, corr, rmse)\n",
    "\n",
    "    # Set axis titles\n",
    "    axs[i].set_title(profiler)\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass0_df_truetaxids = pass0_df[pass0_df['taxonomy_id'].isin(table_artificial_taxcounts['taxid'].values)]\n",
    "pass2_df_truetaxids = pass2_df[pass2_df['taxonomy_id'].isin(table_artificial_taxcounts['taxid'].values)]\n",
    "\n",
    "fig, axs = plt.subplots(1,6, figsize=(18, 3))\n",
    "\n",
    "\n",
    "# Iterate through profilers and calculate Pearson correlation and RMSE\n",
    "for i, profiler in enumerate(LIST_PROFILERS + ['mean']):\n",
    "    # Extract values for pass 0 and pass 2\n",
    "    p0counts = np.log10(1 + pass0_df_truetaxids.loc[:, f'{profiler}_norm'].values)\n",
    "    p2counts = np.log10(1 + pass2_df_truetaxids.loc[:, f'{profiler}_norm'].values)\n",
    "\n",
    "\n",
    "    # Plot vertical line\n",
    "    axs[i].plot([3, 6], [3, 6], c='#bc0000', linestyle='--', linewidth=1.5)\n",
    "\n",
    "    # Scatterplot\n",
    "    sns.scatterplot(x=p0counts, y=p2counts, ax=axs[i])\n",
    "\n",
    "    # Calculate Pearson correlation\n",
    "    corr, _ = pearsonr(p0counts, p2counts)\n",
    "\n",
    "    # Calculate RMSE (Root Mean Squared Error)\n",
    "    rmse = np.sqrt(np.mean((p0counts - p2counts) ** 2))\n",
    "\n",
    "    print(profiler, corr, rmse)\n",
    "\n",
    "    # Set axis titles\n",
    "    axs[i].set_title(profiler)\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see**\n",
    "\n",
    "\n",
    "**ARGUMENTAR QUE NECESITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nominal_stats_sub = df_numerical_stats[(df_numerical_stats['pass'] == 2) & \\\n",
    "                              (df_numerical_stats['mode'].isin([3, 5, 7])) & \\\n",
    "                              (df_numerical_stats['S'] == 10)]\n",
    "\n",
    "df_numerical_stats_sub = df_numerical_stats[(df_numerical_stats['pass'] == 2) & \\\n",
    "                              (df_numerical_stats['mode'].isin([3, 5, 7])) & \\\n",
    "                              (df_numerical_stats['S'] == 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nominal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are relative abundances better predictors than absolute ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the mean a good approximation for the different profilers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prof_couns(df_combo):\n",
    "    # Assuming your data is in a DataFrame called `df`\n",
    "    # Expand the diff_counts column into individual rows for plotting\n",
    "    df_expanded = df_combo.explode('diff_counts')\n",
    "    df_expanded['diff_counts'] = pd.to_numeric(df_expanded['diff_counts'])\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.stripplot(\n",
    "        data=df_expanded,\n",
    "        x='diff_counts',\n",
    "        y='profiler',\n",
    "        jitter=True,  # Adds jitter for better visibility of points\n",
    "        size=5,  # Adjust point size\n",
    "        alpha=0.7  # Slight transparency\n",
    "    )\n",
    "\n",
    "    for profiler, mae, maed in zip(df_combo['profiler'], df_combo['MAE_counts'], df_combo['MAED_counts']):\n",
    "        plt.scatter(mae, profiler, color='#aa00bc', label='_nolegend_', s=100, zorder=3, marker = '|')\n",
    "        plt.plot([mae-maed, mae+maed], [profiler, profiler], color='#aa00bc', label='_nolegend_',)\n",
    "\n",
    "\n",
    "    # Add a vertical line at x=0\n",
    "    plt.axvline(0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Add labels and title\n",
    "    # plt.title('Diff Counts Across Profilers', fontsize=14)\n",
    "    plt.xlabel('Diff Counts', fontsize=12)\n",
    "    plt.ylabel('Profiler', fontsize=12)\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo = df_numerical_stats[(df_numerical_stats['pass'] == 2) & \\\n",
    "                              (df_numerical_stats['mode'].isin([3, 5, 7])) & \\\n",
    "                              (df_numerical_stats['S'] == 10)]\n",
    "df_combo.sort_values(by=['profiler', 'mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo = df_numerical_stats[(df_numerical_stats['pass'] == 2) & \\\n",
    "                              (df_numerical_stats['mode'] == 3) & \\\n",
    "                              (df_numerical_stats['S'] == 10)] # We choose a large number because S in not relevant here (but with small S we may select few datasets)\n",
    "\n",
    "plot_prof_couns(df_combo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo = df_numerical_stats[(df_numerical_stats['pass'] == 2) & \\\n",
    "                              (df_numerical_stats['mode'] == 5) & \\\n",
    "                              (df_numerical_stats['S'] == 10)] # We choose a large number because S in not relevant here (but with small S we may select few datasets)\n",
    "\n",
    "plot_prof_couns(df_combo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo = df_numerical_stats[(df_numerical_stats['pass'] == 2) & \\\n",
    "                              (df_numerical_stats['mode'] == 7) & \\\n",
    "                              (df_numerical_stats['S'] == 10)] # We choose a large number because S in not relevant here (but with small S we may select few datasets)\n",
    "\n",
    "plot_prof_couns(df_combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo = df_numerical_stats[(df_numerical_stats['pass'] == 2) & \\\n",
    "                              (df_numerical_stats['mode'].isin([3, 5, 7])) & \\\n",
    "                              (df_numerical_stats['S'] == 10)]\n",
    "\n",
    "\n",
    "# Create a FacetGrid with one subplot per mode\n",
    "g = sns.FacetGrid(data=df_combo, col=\"mode\", col_wrap=3, height=3, sharex=True, sharey=True)\n",
    "\n",
    "# Map a scatterplot to each subplot, using hue for profilers\n",
    "g.map_dataframe(sns.scatterplot, \"MAE_counts\", \"MAED_counts\", hue=\"profiler\")\n",
    "\n",
    "# Add axis labels and a title\n",
    "g.set_axis_labels(\"MAE Counts\", \"MAED Counts\")\n",
    "g.set_titles(col_template=\"Mode: {col_name}\")\n",
    "g.fig.suptitle(\"Scatter Plot of MAE_counts vs MAED_counts by Mode and Profiler\", fontsize=16, y=1.05)\n",
    "\n",
    "# Adjust legend\n",
    "g.add_legend(title=\"Profiler\", bbox_to_anchor=(1.15, 0.5))\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, choosing the mean value is a good option for several reasons:\n",
    "- In cases where "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EVs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
